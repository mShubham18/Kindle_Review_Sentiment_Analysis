{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mShubham18/Kindle_Review_Sentiment_Analysis/blob/master/Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8hzyZgbOR7hQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8FB-1RLER7hR",
        "outputId": "f0e0b2b5-4860-4195-84de-c2bc028c6ec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Resources/kindle_reviews.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d70e84c3a67f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Resources/kindle_reviews.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Resources/kindle_reviews.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"Resources/kindle_reviews.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tl1C7RwLR7hR"
      },
      "outputs": [],
      "source": [
        "dropcol = df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfw6cP71R7hR"
      },
      "outputs": [],
      "source": [
        "df.drop(dropcol[0],inplace=True,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwtk_Y2VR7hS"
      },
      "outputs": [],
      "source": [
        "df=df[[\"overall\",\"reviewText\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KSZ8sskR7hS",
        "outputId": "aa313af4-8c54-4b83-f479-d1af909ee361"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>reviewText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>I enjoy vintage books and movies so I enjoyed ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>This book is a reissue of an old one; the auth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>This was a fairly interesting read.  It had ol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>I'd never read any of the Amy Brewster mysteri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>If you like period pieces - clothing, lingo, y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982614</th>\n",
              "      <td>5</td>\n",
              "      <td>Yasss hunny! This is a great read. That Dre is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982615</th>\n",
              "      <td>5</td>\n",
              "      <td>I ENJOYED THIS BOOK FROM BEGINNING TO END NOW ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982616</th>\n",
              "      <td>5</td>\n",
              "      <td>Great book! Cherika was a fool. She let that m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982617</th>\n",
              "      <td>5</td>\n",
              "      <td>When I say this was an excellent book please b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982618</th>\n",
              "      <td>5</td>\n",
              "      <td>This book was everything. I just hope Alexus w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>982619 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        overall                                         reviewText\n",
              "0             5  I enjoy vintage books and movies so I enjoyed ...\n",
              "1             4  This book is a reissue of an old one; the auth...\n",
              "2             4  This was a fairly interesting read.  It had ol...\n",
              "3             5  I'd never read any of the Amy Brewster mysteri...\n",
              "4             4  If you like period pieces - clothing, lingo, y...\n",
              "...         ...                                                ...\n",
              "982614        5  Yasss hunny! This is a great read. That Dre is...\n",
              "982615        5  I ENJOYED THIS BOOK FROM BEGINNING TO END NOW ...\n",
              "982616        5  Great book! Cherika was a fool. She let that m...\n",
              "982617        5  When I say this was an excellent book please b...\n",
              "982618        5  This book was everything. I just hope Alexus w...\n",
              "\n",
              "[982619 rows x 2 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fA5oixAR7hS"
      },
      "outputs": [],
      "source": [
        "df=df[[\"overall\",\"reviewText\"]]\n",
        "\n",
        "df = df.rename(columns=\n",
        "    {\n",
        "        \"overall\":\"rating\",\n",
        "        \"reviewText\":\"message\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d7PBU_vR7hS"
      },
      "source": [
        "TrainTestSplit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcewu_eOR7hT"
      },
      "source": [
        "-- Categories:<br>\n",
        "Any review rated < 3 --> negative<br>\n",
        "Any review rated > 3 --> positive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Stb8Fw1_R7hT",
        "outputId": "683573e5-b75f-45f0-be88-71d64ed8b193"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"None of [Index(['overall', 'reviewText'], dtype='object')] are in the [columns]\"",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreviewText\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m\n\u001b[0;32m      4\u001b[0m     {\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviewText\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m     }\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x:\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\HP\\OneDrive\\Documents\\Kindle_Sentiment_Analysis\\kindle\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\HP\\OneDrive\\Documents\\Kindle_Sentiment_Analysis\\kindle\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\HP\\OneDrive\\Documents\\Kindle_Sentiment_Analysis\\kindle\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['overall', 'reviewText'], dtype='object')] are in the [columns]\""
          ]
        }
      ],
      "source": [
        "df=df[[\"overall\",\"reviewText\"]]\n",
        "\n",
        "df = df.rename(columns=\n",
        "    {\n",
        "        \"overall\":\"rating\",\n",
        "        \"reviewText\":\"message\"\n",
        "    }\n",
        ")\n",
        "\n",
        "df[\"rating\"]=df[\"rating\"].apply(lambda x:0 if x<3 else 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6m9qFysR7hT",
        "outputId": "24867d28-fd14-472c-ee37-1d04041bdb11"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "rating\n",
              "1    925471\n",
              "0     57148\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"rating\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hS2xIiTR7hT"
      },
      "outputs": [],
      "source": [
        "# Lowercasing the words\n",
        "df[\"message\"]=df[\"message\"].apply(lambda x: str(x).lower())"
      ]
<<<<<<< HEAD
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Word2Vec or BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(ngram_range=(1,2))\n",
    "X_train_bow = bow.fit_transform(X_train)\n",
    "X_test_bow = bow.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************Training Score*****************************************\n",
      "bow_accuracy_score_train:  0.9536413635286473\n",
      "bow_f1_score_train:  0.9759630207277203\n",
      "bow_roc_auc_score_train:  0.60648565005485\n",
      "*****************************************Testing Score*****************************************\n",
      "bow_accuracy_score_test:  0.9441289613482323\n",
      "bow_f1_score_test:  0.9712253091488492\n",
      "bow_roc_auc_score_test:  0.510735937049366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_bow,y_train)\n",
    "y_pred_train = model.predict(X_train_bow)\n",
    "y_pred_test = model.predict(X_test_bow)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,f1_score,roc_auc_score\n",
    "print(\"*****************************************Training Score*****************************************\")\n",
    "bow_accuracy_score_train = accuracy_score(y_train,y_pred_train)\n",
    "bow_f1_score_train = f1_score(y_train,y_pred_train)\n",
    "bow_roc_auc_score_train = roc_auc_score(y_train,y_pred_train)\n",
    "print(\"bow_accuracy_score_train: \",bow_accuracy_score_train)\n",
    "print(\"bow_f1_score_train: \",bow_f1_score_train)\n",
    "print(\"bow_roc_auc_score_train: \",bow_roc_auc_score_train)\n",
    "print(\"*****************************************Testing Score*****************************************\")\n",
    "bow_accuracy_score_test = accuracy_score(y_test,y_pred_test)\n",
    "bow_f1_score_test = f1_score(y_test,y_pred_test)\n",
    "bow_roc_auc_score_test = roc_auc_score(y_test,y_pred_test)\n",
    "print(\"bow_accuracy_score_test: \",bow_accuracy_score_test)\n",
    "print(\"bow_f1_score_test: \",bow_f1_score_test)\n",
    "print(\"bow_roc_auc_score_test: \",bow_roc_auc_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************Training Score*****************************************\n",
      "tfidf_accuracy_score_train:  0.9410918057144685\n",
      "tfidf_f1_score_train:  0.9696410254873328\n",
      "ifidf_roc_auc_score_train:  0.5026681282573361\n",
      "*****************************************Testing Score*****************************************\n",
      "tfidf_accuracy_score_test:  0.9427720448053842\n",
      "tfidf_f1_score_test:  0.9705387205387206\n",
      "tfidf_roc_auc_score_test:  0.5011057870903728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf,y_train)\n",
    "y_pred_train = model.predict(X_train_tfidf)\n",
    "y_pred_test = model.predict(X_test_tfidf)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,f1_score,roc_auc_score\n",
    "print(\"*****************************************Training Score*****************************************\")\n",
    "tfidf_accuracy_score_train = accuracy_score(y_train,y_pred_train)\n",
    "tfidf_f1_score_train = f1_score(y_train,y_pred_train)\n",
    "ifidf_roc_auc_score_train = roc_auc_score(y_train,y_pred_train)\n",
    "print(\"tfidf_accuracy_score_train: \",tfidf_accuracy_score_train)\n",
    "print(\"tfidf_f1_score_train: \",tfidf_f1_score_train)\n",
    "print(\"ifidf_roc_auc_score_train: \",ifidf_roc_auc_score_train)\n",
    "print(\"*****************************************Testing Score*****************************************\")\n",
    "tfidf_accuracy_score_test = accuracy_score(y_test,y_pred_test)\n",
    "tfidf_f1_score_test = f1_score(y_test,y_pred_test)\n",
    "tfidf_roc_auc_score_test = roc_auc_score(y_test,y_pred_test)\n",
    "print(\"tfidf_accuracy_score_test: \",tfidf_accuracy_score_test)\n",
    "print(\"tfidf_f1_score_test: \",tfidf_f1_score_test)\n",
    "print(\"tfidf_roc_auc_score_test: \",tfidf_roc_auc_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INLa8fGkR7hU",
        "outputId": "f4196416-1876-4625-bb2d-f841b9b2e483"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>i enjoy vintage books and movies so i enjoyed ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>this book is a reissue of an old one; the auth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>this was a fairly interesting read.  it had ol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>i'd never read any of the amy brewster mysteri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>if you like period pieces - clothing, lingo, y...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   rating                                            message\n",
              "0       1  i enjoy vintage books and movies so i enjoyed ...\n",
              "1       1  this book is a reissue of an old one; the auth...\n",
              "2       1  this was a fairly interesting read.  it had ol...\n",
              "3       1  i'd never read any of the amy brewster mysteri...\n",
              "4       1  if you like period pieces - clothing, lingo, y..."
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e5rHvQnR7hU",
        "outputId": "404ccaba-6692-43f2-a055-5f6543b9a61b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15520\\285228860.py:28: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a filename than HTML or XML.\n",
            "\n",
            "If you meant to use Beautiful Soup to parse the contents of a file on disk, then something has gone wrong. You should open the file first, using code like this:\n",
            "\n",
            "    filehandle = open(your filename)\n",
            "\n",
            "You can then feed the open filehandle into Beautiful Soup instead of using the filename.\n",
            "\n",
            "However, if you want to parse some data that happens to look like a filename, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import MarkupResemblesLocatorWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
            "    \n",
            "  df[\"message\"] = df[\"message\"].map(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n"
          ]
        }
      ],
      "source": [
        "# Remove special characters\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#df=df[[\"overall\",\"reviewText\"]]\n",
        "\n",
        "df = df.rename(columns=\n",
        "    {\n",
        "        \"overall\":\"rating\",\n",
        "        \"reviewText\":\"message\"\n",
        "    }\n",
        ")\n",
        "\n",
        "df[\"rating\"]=df[\"rating\"].apply(lambda x:0 if x<3 else 1)\n",
        "\n",
        "# Lowercasing the words\n",
        "df[\"message\"]=df[\"message\"].apply(lambda x: str(x).lower())\n",
        "\n",
        "\n",
        "df[\"message\"] =df[\"message\"].apply(lambda sentence: \" \".join(filter(lambda word: word not in stop_words, sentence.split())))\n",
        "\n",
        "#Remove url\n",
        "\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: re.sub(r\"(http|https|ftp|ssh)://[\\w_-]+(?:\\.[\\w_-]+)+[\\w.,@?^=%&:/~+#-]*\", \"\", x))\n",
        "\n",
        "# Remove any HTML keywords or tags\n",
        "df[\"message\"] = df[\"message\"].map(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
        "\n",
        "# Remove any additional whitespaces\n",
        "df[\"message\"]=df[\"message\"].apply(lambda x: \" \".join(x.split()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLWp4SPdR7hU"
      },
      "source": [
        "Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDp_SGkdR7hU"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By11Iys3R7hU",
        "outputId": "ab5d14b5-c1ba-4cc0-ce04-fe5bb45cba34"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>enjoy vintage books movies enjoyed reading boo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>book reissue old one; author born 1910. era of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>fairly interesting read. old- style terminolog...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>never read amy brewster mysteries one.. really...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>like period pieces - clothing, lingo, enjoy my...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982614</th>\n",
              "      <td>1</td>\n",
              "      <td>yasss hunny! great read. dre mess, cherika ref...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982615</th>\n",
              "      <td>1</td>\n",
              "      <td>enjoyed book beginning end far lex hoe sneaky ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982616</th>\n",
              "      <td>1</td>\n",
              "      <td>great book! cherika fool. let man get away muc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982617</th>\n",
              "      <td>1</td>\n",
              "      <td>say excellent book please believe definitely p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982618</th>\n",
              "      <td>1</td>\n",
              "      <td>book everything. hope alexus wise move on. law...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>982619 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        rating                                            message\n",
              "0            1  enjoy vintage books movies enjoyed reading boo...\n",
              "1            1  book reissue old one; author born 1910. era of...\n",
              "2            1  fairly interesting read. old- style terminolog...\n",
              "3            1  never read amy brewster mysteries one.. really...\n",
              "4            1  like period pieces - clothing, lingo, enjoy my...\n",
              "...        ...                                                ...\n",
              "982614       1  yasss hunny! great read. dre mess, cherika ref...\n",
              "982615       1  enjoyed book beginning end far lex hoe sneaky ...\n",
              "982616       1  great book! cherika fool. let man get away muc...\n",
              "982617       1  say excellent book please believe definitely p...\n",
              "982618       1  book everything. hope alexus wise move on. law...\n",
              "\n",
              "[982619 rows x 2 columns]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7UFHRoTR7hU"
      },
      "outputs": [],
      "source": [
        "def lemmatize_word(text):\n",
        "   return \" \".join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI82jVwpR7hV",
        "outputId": "dfe77f6e-f150-4a33-cca8-359d2c7b8ecb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         enjoy vintage book movie enjoyed reading book....\n",
              "1         book reissue old one; author born 1910. era of...\n",
              "2         fairly interesting read. old- style terminolog...\n",
              "3         never read amy brewster mystery one.. really h...\n",
              "4         like period piece - clothing, lingo, enjoy mys...\n",
              "                                ...                        \n",
              "982614    yasss hunny! great read. dre mess, cherika ref...\n",
              "982615    enjoyed book beginning end far lex hoe sneaky ...\n",
              "982616    great book! cherika fool. let man get away muc...\n",
              "982617    say excellent book please believe definitely p...\n",
              "982618    book everything. hope alexus wise move on. law...\n",
              "Name: message, Length: 982619, dtype: object"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"message\"].apply(lambda x: lemmatize_word(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66YTrx7oR7hV",
        "outputId": "8cdc0ad8-3038-4bf4-a6b7-fe835584f851"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>enjoy vintage books movies enjoyed reading boo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>book reissue old one; author born 1910. era of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>fairly interesting read. old- style terminolog...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>never read amy brewster mysteries one.. really...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>like period pieces - clothing, lingo, enjoy my...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982614</th>\n",
              "      <td>1</td>\n",
              "      <td>yasss hunny! great read. dre mess, cherika ref...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982615</th>\n",
              "      <td>1</td>\n",
              "      <td>enjoyed book beginning end far lex hoe sneaky ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982616</th>\n",
              "      <td>1</td>\n",
              "      <td>great book! cherika fool. let man get away muc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982617</th>\n",
              "      <td>1</td>\n",
              "      <td>say excellent book please believe definitely p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982618</th>\n",
              "      <td>1</td>\n",
              "      <td>book everything. hope alexus wise move on. law...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>982619 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        rating                                            message\n",
              "0            1  enjoy vintage books movies enjoyed reading boo...\n",
              "1            1  book reissue old one; author born 1910. era of...\n",
              "2            1  fairly interesting read. old- style terminolog...\n",
              "3            1  never read amy brewster mysteries one.. really...\n",
              "4            1  like period pieces - clothing, lingo, enjoy my...\n",
              "...        ...                                                ...\n",
              "982614       1  yasss hunny! great read. dre mess, cherika ref...\n",
              "982615       1  enjoyed book beginning end far lex hoe sneaky ...\n",
              "982616       1  great book! cherika fool. let man get away muc...\n",
              "982617       1  say excellent book please believe definitely p...\n",
              "982618       1  book everything. hope alexus wise move on. law...\n",
              "\n",
              "[982619 rows x 2 columns]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq6SZ1XOR7hV",
        "outputId": "478c8eea-cc65-49a8-b269-8e997fc481c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 982619 entries, 0 to 982618\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count   Dtype \n",
            "---  ------   --------------   ----- \n",
            " 0   rating   982619 non-null  int64 \n",
            " 1   message  982619 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 15.0+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEmF9-MVR7hV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(df[\"message\"],df[\"rating\"],test_size=0.3,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQlIMttaR7hV",
        "outputId": "b9380bf2-f8aa-46c9-b0b3-c52bf3b63900"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "967979    book gets back basics natural remedies parents...\n",
              "674481    loved romance tom cooper age gap didn’t seem m...\n",
              "786555    suffered panic anxiety attacks last ten years ...\n",
              "334017    dunnoi expect bit fun story found silly, much ...\n",
              "511689    short story good start. gotta love naunie baby...\n",
              "                                ...                        \n",
              "259178    renowned cartoonist brian platt uses creative ...\n",
              "365838    delightful australian coastal romance splash s...\n",
              "131932    again, wondering read another book. book terri...\n",
              "671155    ian gabby loved story!!! ian hot sexy gabby br...\n",
              "121958    fascinating antarctic setting good romance. em...\n",
              "Name: message, Length: 687833, dtype: object"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-p6ua4rR7hV"
      },
      "source": [
        "Applying Word2Vec or BOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0Mdd8SkR7hV",
        "outputId": "5f15a39f-8e3a-4e14-90ea-91d3620ce91a"
      },
      "outputs": [
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 53.1 TiB for an array with shape (687833, 10607968) and data type int64",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[45], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m      2\u001b[0m bow \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m X_train_bow \u001b[38;5;241m=\u001b[39m \u001b[43mbow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m10000\u001b[39m]\n\u001b[0;32m      4\u001b[0m X_test_bow \u001b[38;5;241m=\u001b[39m bow\u001b[38;5;241m.\u001b[39mtransform(X_test)\u001b[38;5;241m.\u001b[39mtoarray()[:\u001b[38;5;241m10000\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\HP\\OneDrive\\Documents\\Kindle_Sentiment_Analysis\\kindle\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1170\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1170\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\HP\\OneDrive\\Documents\\Kindle_Sentiment_Analysis\\kindle\\Lib\\site-packages\\scipy\\sparse\\_base.py:1367\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 53.1 TiB for an array with shape (687833, 10607968) and data type int64"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "bow = CountVectorizer(ngram_range=(1,2))\n",
        "X_train_bow = bow.fit_transform(X_train).toarray()[:10000]\n",
        "X_test_bow = bow.transform(X_test).toarray()[:10000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJLwswCRR7hV"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
        "X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
        "X_test_tfidf = tfidf.transform(X_test).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ20q_liR7hW"
      },
      "outputs": [],
      "source": [
        "from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXMARtd5R7hW",
        "outputId": "83474c31-4ca4-4288-bcdd-121c4def950f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(982619, 2)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgjKDPd4R7hW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
>>>>>>> 850f967855c3f7e67451031cdc33a64c40e30cb0
  },
  "nbformat": 4,
  "nbformat_minor": 0
}